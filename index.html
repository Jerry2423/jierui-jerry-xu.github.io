<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Jierui (Jerry) Xu</title>

    <meta name="author" content="Jierui (Jerry) Xu">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Jierui (Jerry) Xu
                </p>
                <p>
                I'm a junior undergraduate student in Computer Science at <a href="https://www.wisc.edu/">University of Wisconsin–Madison</a>, pursuing my Bachelor's degree.
                I previously studied at <a href="https://www.shanghaitech.edu.cn/eng/">ShanghaiTech University</a> in Shanghai, China.
                My research interest lies in building high-performance and scalable systems for LLMs, focusing on software-hardware co-design for novel AI accelerator architectures to minimize latency and maximize throughput at scale.
                </p>
                <p style="text-align:center">
                  <a href="mailto:xjr2423@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/JieruiXu-CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="data/JieruiXu-bio.txt">Bio</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/jierui-xu-jr">LinkedIn</a> &nbsp;/&nbsp;
                  <a href="https://github.com/Jerry2423">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/profile.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/profile.jpeg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>

          <!-- Publications Section -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Publications</h2>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

    <!-- NeuronMM Publication -->
    <tr>
      <td style="padding:16px;width:100%;vertical-align:middle">
        <span class="papertitle">NeuronMM: High-Performance Matrix Multiplication for LLM Inference on AWS Trainium</span>
        <br>
        Dinghong Song*, <strong>Jierui Xu*</strong>, Weichu Yang, Pengfei Su, Dong Li
        <br>
        <em>Submitted to European Conference on Computer Systems (EuroSys)</em>, 2026
        <br>
        <a href="https://github.com/Jerry2423/SVD-Flash">code</a>
      </td>
    </tr>

          </tbody></table>

          <!-- Awards Section -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Awards</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <strong><a href="https://github.com/asplos-contest/2025/blob/main/OPTNKI.md">ASPLOS / EuroSys 2025 Programming Contest</a> - Second Place Winner</strong><br>
                <em>April 2025, Rotterdam, The Netherlands</em><br>
                Awarded for developing the fastest inference implementation of the Llama 3.2 1B model on AWS Trainium hardware by designing highly-optimized custom kernels using the Neuron Kernel Interface (NKI).
              </td>
            </tr>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <strong><a href="https://registrar.wisc.edu/deanslist">Dean's List</a> - College of Letters & Science</strong><br>
                <em>Spring 2025, University of Wisconsin–Madison</em><br>
                Recognized for high scholastic achievement with requisite GPA of 3.850+ for junior/senior level coursework.
              </td>
            </tr>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <strong><a href="https://pages.cs.wisc.edu/~dieter/ICPC/hall-of-fame.html">ICPC 2024 North Central North America Regional Contest</a> - Top 10</strong><br>
                <em>November 2024, Madison, WI</em><br>
                Ranked in the Top 10 out of 250+ contestants. Collaborated with teammates and solved algorithm and data structure problems in real-time under tight time constraints.
              </td>
            </tr>
          </tbody></table>

          <!-- Experience Section -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Experience</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <strong>Research Assistant, University of California Merced</strong><br>
                <em>January 2025 - October 2025 (Remote)</em><br>
                <em><strong>Advisor: Prof. Dong Li</strong>, in Collaboration with Amazon Web Services (AWS)</em><br>
                • Optimized Llama 3.2 1B inference with custom kernels on AWS Trainium, achieving 78% latency reduction (6.43s → 1.40s) and 4.8x throughput (102.60 → 494.39 tokens/s) vs. PyTorch baseline<br>
                • Compressed Llama and Qwen LLMs via SVD and applied LoRA fine-tuning to restore performance, limiting mean accuracy drop to ≤0.10 across 9 datasets<br>
                • Redesigned fused attention kernel with tiling techniques to optimize tensor layouts on SBUF and PSUM memory, expanding maximum sequence length capacity by 7.8× (640 to 5k tokens)
              </td>
            </tr>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <strong>Software Engineer Intern, Amazon</strong><br>
                <em>May 2025 - August 2025, Shanghai, China</em><br>
                • Designed and implemented an LLM-based search keywords recommendation system analyzing real-time customer behavior to generate search suggestions, driving $7.12MM annualized operating profit<br>
                • Automated an LLM inference platform on AWS ECS clusters with Triton server and vLLM backend, achieving sub-100ms latency at scale<br>
                • Developed a daily automated Spark SQL pipeline processing 5M+ customer clickstream events to analyze shopping patterns, saving 4+ hours of manual maintenance per week
              </td>
            </tr>
          </tbody></table>

          <!-- Projects Section -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Projects</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <span class="papertitle">Shakespeare GPT</span>
                <br>
                <em>PyTorch, HuggingFace Datasets</em>
                <br>
                <a href="https://colab.research.google.com/drive/1pnxjf9jDXOur2q9Wz1Gfox_ERqV3f5Gk?usp=sharing">colab</a>
                <br>
                • Engineered a complete GPT framework from scratch in PyTorch, implementing a modern Transformer architecture (w/ RoPE, RMSNorm), a BPE tokenizer, and an AdamW optimizer.<br>
                • Trained the model on a Shakespearean corpus (1.5 hrs, V100) to generate Shakespeare-style text, achieving a 3.55 validation loss.
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <span class="papertitle">CUDA LLM Inference Engine</span>
                <br>
                <em>C++17, CUDA, Jenkins</em>
                <br>
                <a href="https://github.com/Jerry2423/gpt_cpp">code</a>
                <br>
                • Developed a CUDA-based command-line application to run LLM, generating texts with user-defined prompts.<br>
                • Developed self-attention kernel, and used cuBLAS for optimized matrix operations, boosting performance from 110 tokens per second to 520 tokens per second on NVIDIA RTX 4050.<br>
                • Built an automated test suite with memory error detection support based on Valgrind and Jenkins, discovering and fixing 18 bugs.
              </td>
            </tr>

          </tbody></table>

          <!-- Skills Section -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Technical Skills</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="colored-box" style="background-color: #fff0f5;">
                  <strong>Programming Languages</strong>
                </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <strong>Systems & Performance:</strong> Python, C++, C, CUDA<br>
                <strong>General Purpose:</strong> Java, JavaScript/TypeScript<br>
                <strong>Web & Data:</strong> SQL, HTML, CSS, LaTeX
              </td>
            </tr>
            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="colored-box" style="background-color: #fff0f5;">
                  <strong>ML/AI Frameworks</strong>
                </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <strong>Deep Learning:</strong> PyTorch, TensorFlow, Hugging Face Transformers<br>
                <strong>Optimization:</strong> CUDA, Triton, TensorRT, vLLM, ONNX<br>
                <strong>LLM Infrastructure:</strong> Model optimization, distributed training, inference acceleration
              </td>
            </tr>
            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="colored-box" style="background-color: #fff0f5;">
                  <strong>Hardware & Systems</strong>
                </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <strong>AI Accelerators:</strong> AWS Trainium, GPUs, TPUs<br>
                <strong>Performance Optimization:</strong> CUDA programming, kernel optimization, memory management<br>
                <strong>Specializations:</strong> Software-hardware co-design, distributed computing, high-performance computing
              </td>
            </tr>
          </tbody></table>

          <!-- Education Section -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Education</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="colored-box" style="background-color: #f5f5dc;">
                  <strong>Current Student</strong>
                </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <strong>University of Wisconsin–Madison</strong><br>
                <em>Bachelor of Science in Computer Science, September 2024 - May 2027</em><br>
                <strong>GPA:</strong> 4.0 / 4.0<br>
                Focus: High-performance systems, software-hardware co-design, AI accelerator architectures
              </td>
            </tr>
            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="colored-box" style="background-color: #f5f5dc;">
                  <strong>Previous Education</strong>
                </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <strong>ShanghaiTech University</strong><br>
                <em>Bachelor of Engineering in Computer Science, September 2022 - May 2024</em><br>
                <strong>GPA:</strong> 3.9 / 4.0<br>
                Foundation in computer science fundamentals, algorithms, and system design
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:center;font-size:small;">
                  Website template borrowed from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
